{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "pd.set_option('display.max_columns', 100)  \n",
    "\n",
    "from azure.datalake.store import core, lib, multithread\n",
    "import os\n",
    "import pathlib\n",
    "\n",
    "import argparse\n",
    "import warnings  \n",
    "import time\n",
    "from scipy import stats\n",
    "from datetime import datetime as dt\n",
    "import collections\n",
    "import pickle\n",
    "import tempfile\n",
    "\n",
    "import pickle\n",
    "import os\n",
    "import azureml.core\n",
    "from azureml.core.compute import ComputeTarget, DataFactoryCompute\n",
    "from azureml.exceptions import ComputeTargetException\n",
    "from azureml.core import Workspace, Experiment\n",
    "from azureml.pipeline.core import Pipeline\n",
    "from azureml.core.datastore import Datastore\n",
    "from azureml.data.data_reference import DataReference\n",
    "from azureml.core import Dataset\n",
    "\n",
    "from azureml.pipeline.steps import DataTransferStep\n",
    "\n",
    "from msrest.exceptions import HttpOperationError\n",
    "\n",
    "\n",
    "from azureml.pipeline.core import Pipeline, PipelineRun, PipelineData, PipelineParameter\n",
    "from azureml.pipeline.steps import PythonScriptStep\n",
    "from azureml.data.datapath import DataPath, DataPathComputeBinding\n",
    "from azureml.core import Workspace, Experiment, Datastore\n",
    "\n",
    "from azureml.widgets import RunDetails\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "!pip install 'xlrd>= 1.0.0.'\n",
    "!pip install haversine\n",
    "!pip install shapely\n",
    "!pip install googlemaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.authentication import ServicePrincipalAuthentication\n",
    "\n",
    "sp = ServicePrincipalAuthentication(tenant_id=\"...\", # tenantID\n",
    "                                    service_principal_id=\"...\", # clientId\n",
    "                                    service_principal_password=\"...\") # clientSecret\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace\n",
    "\n",
    "subscription_id = '...'\n",
    "resource_group = '...'\n",
    "workspace_name = '...'\n",
    "\n",
    "\n",
    "ws = Workspace(subscription_id, resource_group, workspace_name, auth = sp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datastore rbru already exists\n"
     ]
    }
   ],
   "source": [
    "blob_datastore_name='rbru' # Name of the datastore to workspace\n",
    "\n",
    "container_name=os.getenv(\"BLOB_CONTAINER\", \"rbru\") # Name of Azure blob container\n",
    "account_name=os.getenv(\"BLOB_ACCOUNTNAME\", \"qrbexpdlg2weu01\") # Storage account name\n",
    "\n",
    "# Storage account access key\n",
    "account_key=os.getenv(\"BLOB_ACCOUNT_KEY\", \"...\") \n",
    "\n",
    "if blob_datastore_name not in ws.datastores.keys():\n",
    "    blob_datastore = Datastore.register_azure_blob_container(workspace=ws, \n",
    "                                                         datastore_name=blob_datastore_name, \n",
    "                                                         container_name=container_name, \n",
    "                                                         account_name=account_name,\n",
    "                                                         account_key=account_key)\n",
    "    print(\"Registered datastore with name: %s\" % blob_datastore_name)\n",
    "else:\n",
    "    print(\"Datastore {} already exists\".format(blob_datastore_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['workspaceartifactstore', 'eudata', 'rbru', 'rbrumassdl', 'rbrucisdl', 'rb_health_russia', 'azureml_globaldatasets', 'workspaceblobstore', 'workspacefilestore'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ws.datastores.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "blob_datastore_source = ws.datastores['rb_health_russia']\n",
    "blob_datastore = ws.datastores['rbru']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipleline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mass_explode\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Create a folder \n",
    "script_folder = 'mass_explode'\n",
    "os.makedirs(script_folder, exist_ok=True)\n",
    "\n",
    "print(script_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Environment\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "from azureml.core.runconfig import RunConfiguration\n",
    "\n",
    "env = Environment(\"pipeline-env\")\n",
    "env.python.user_managed_dependencies = False # Let Azure ML manage dependencies\n",
    "env.docker.enabled = True # Use a docker container\n",
    "\n",
    "dependencies = CondaDependencies.create(conda_packages=['scikit-learn','pandas'],\n",
    "                                             pip_packages=['azureml-defaults', 'xlrd', 'geopy',\n",
    "                                                           'googlemaps', 'haversine', 'shapely', \n",
    "                                                           'azureml-dataprep[pandas]', 'openpyxl',\n",
    "                                                            'ortools==6.7.4973'])\n",
    "env.python.conda_dependencies = dependencies\n",
    "\n",
    "env.register(workspace=ws)\n",
    "registered_env = Environment.get(ws, 'pipeline-env')\n",
    "pipeline_run_config = RunConfiguration()\n",
    "\n",
    "pipeline_run_config.target =  'fff555'  #aaa111  \n",
    "pipeline_run_config.environment = registered_env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _final poi_step_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mass_explode/poi_volga.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile $script_folder/poi_volga.py\n",
    "from azureml.core import Run\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os, argparse, time, warnings, re \n",
    "import urllib\n",
    "import requests\n",
    "import geopy.distance\n",
    "run = Run.get_context()\n",
    "\n",
    "def data_prep(store_data, poi_data):\n",
    "    \n",
    "    poi_data.fillna(poi_data[[i for i in poi_data.columns if  i.islower()]  + ['Rating']].fillna(0), inplace=True) #fillna cols with google types\n",
    "\n",
    "    store_data['Address'] = np.where((store_data['FiasAccuracy'] =='Корректный') & (store_data['FiasAddress'].notnull()), \n",
    "                             store_data['FiasAddress'], store_data['SourceAddress'])\n",
    "\n",
    "    regions = ['Moscow', 'Center', 'Ural', 'Far East', 'Volga', 'South', 'North-West', 'Siberia']\n",
    "#     store_data = store_data[store_data['Macroregion'].isin(['Volga'])]\n",
    "\n",
    "    store_data=store_data.loc[(store_data['Active (Y/N)']==True) & (store_data['Channel'].isin(['LKA','NKA']))]\n",
    "    print(store_data.shape)\n",
    "\n",
    "    store_data = store_data[pd.notnull(store_data['Store ID'])]\n",
    "    store_data['Store ID'] =store_data['Store ID'].astype(int)\n",
    "    store_data['Store ID'] =store_data['Store ID'].apply(lambda x:str(x)+\"_\")\n",
    "\n",
    "    store_data_final=store_data.drop_duplicates(['Store ID','City'])\n",
    "    store_data_final=store_data_final.loc[store_data_final[\"Format\"].isin(\n",
    "                ['Super', 'Drug', 'Discounter', 'Kiosk', 'Hyper', 'Other', 'Alco Minimarket', \n",
    "                 'WS', 'Convenience Store', 'Ecomm', 'Pavilion','Petrol Station', 'Unknown']),:]\n",
    "\n",
    "    store_data  = store_data[['Store ID','Address', 'Lat/Long', 'Channel', 'FiasAccuracy', 'Macroregion']] \n",
    "    store_data[['Широта','Долгота']]=store_data['Lat/Long'].str.split('/',expand=True).apply(lambda x: x.str.strip().astype('float'))\n",
    "    store_data['Coordinates'] =  list(zip(store_data['Широта'], store_data['Долгота']))\n",
    "\n",
    "    store_data.drop('Lat/Long', inplace =True, axis=1)\n",
    "    store_data = store_data.dropna(subset=['Долгота', 'Широта']) #remove where lat, long = NaN\n",
    "    print(store_data.shape)\n",
    "    l = list(set(store_data['Store ID']) - set(poi_data['Store ID']))\n",
    "    print(len(l))\n",
    "    l =l[:200]         #  batch of 200 stores per run\n",
    "    \n",
    "    store_data = store_data[store_data['Store ID'].isin(l)]\n",
    "    return store_data\n",
    "\n",
    "\n",
    "#add Points of interest in radius 300 metres, from all pages \n",
    "def add_poi(df):\n",
    "    main_api = 'https://maps.googleapis.com/maps/api/place/nearbysearch/json?'\n",
    "\n",
    "    key ='...' #password\n",
    "    radius = 300\n",
    "    language ='ru'\n",
    "\n",
    "    data =[]\n",
    "    for i in  range(0, df.shape[0]): \n",
    " \n",
    "        print('Iteration POI:', i,  df['Store ID'].iloc[i])\n",
    "        coords = df['Coordinates'].iloc[i]\n",
    "        url = main_api + urllib.parse.urlencode({'location': re.sub(r'\\s*, \\s*', ',', str(coords)[1:-1]),\n",
    "                                                 'key': key, 'radius': radius, 'language': language})\n",
    "        time.sleep(2)\n",
    "        data_json = requests.get(url).json()\n",
    "        json_status = data_json['status']\n",
    "        \n",
    "        if json_status =='OK':\n",
    "            poi=[]\n",
    "            names =[]\n",
    "\n",
    "            for k in data_json['results']:\n",
    "                names.append(k['name'])\n",
    "\n",
    "                for j in k['types']:\n",
    "                    poi.append(j)\n",
    "                    \n",
    "            data.append(pd.merge(df.reset_index(drop= True),\n",
    "                                 pd.DataFrame({k: poi.count(k) for k in poi}, index = [i]), \n",
    "                                 left_index=True, right_index=True))\n",
    "             \n",
    "            try:\n",
    "                next_page_token = data_json[\"next_page_token\"]\n",
    "                \n",
    "                while data_json[\"next_page_token\"]:\n",
    "\n",
    "                    url_nextpage = main_api + urllib.parse.urlencode({'key': key, \n",
    "                                                            'pagetoken': next_page_token})\n",
    "                    #print(url_nextpage)\n",
    "                    time.sleep(2)\n",
    "                    data_json = requests.get(url_nextpage).json()\n",
    "\n",
    "                    if data_json['status'] =='OK':\n",
    "                        \n",
    "                        for k in data_json['results']:\n",
    "                            names.append(k['name'])\n",
    "\n",
    "                            for j in k['types']:\n",
    "                                poi.append(j)\n",
    "                    #print('poi on next page:', {k: poi.count(k) for k in poi})\n",
    "\n",
    "                    try:\n",
    "                        next_page_token = data_json[\"next_page_token\"]\n",
    "                    except KeyError:\n",
    "                        break\n",
    "                        \n",
    "                    continue\n",
    "                    \n",
    "            except KeyError:\n",
    "                continue\n",
    "                \n",
    "        data.append(pd.merge(df.reset_index(drop= True),\n",
    "                       pd.DataFrame({k: poi.count(k) for k in poi}, index = [i]), \n",
    "                       left_index=True, right_index=True))\n",
    "\n",
    "\n",
    "    d =pd.concat(data, ignore_index=True)    \n",
    "\n",
    "    cols = [i for i in d.columns if not i.islower()] + [i for i in d.columns if i.islower()]\n",
    "    d = d[cols]\n",
    "    d.drop_duplicates(['Store ID'], keep='last',inplace=True)\n",
    "    d =d.reset_index(drop= True)\n",
    "    d.fillna(d[[i for i in d.columns if  i.islower()]].fillna(0), inplace=True) #fillna cols with google types\n",
    "    return d\n",
    "\n",
    "\n",
    "\n",
    "#add features: 'Opening_hours', 'Rating', 'User_ratings_total', 'Price_level'\n",
    "\n",
    "def add_features(df):\n",
    "    main_api = 'https://maps.googleapis.com/maps/api/place/findplacefromtext/json?'\n",
    "\n",
    "    key ='...'\n",
    "    language ='ru'\n",
    "\n",
    "    data=[]\n",
    "    lat, lng, price_level, rating, user_ratings_total, opening_hours =([] for i in range(6))\n",
    "\n",
    "    for i in  range(0, df.shape[0]):  \n",
    "        #print('Query: ',i)\n",
    "        input = df['Address'].iloc[i]\n",
    "        url = main_api + urllib.parse.urlencode({'input':input, 'inputtype': 'textquery',\n",
    "                                                 'key': key,'language': language,\n",
    "                                                 'fields': 'place_id,photos,formatted_address,name,rating,price_level,opening_hours,user_ratings_total,geometry'})\n",
    "\n",
    "        time.sleep(2)\n",
    "        data_json = requests.get(url).json()\n",
    "        # print(data_json)\n",
    "\n",
    "        if data_json['status'] =='OK':\n",
    "\n",
    "            try:\n",
    "                place_id = data_json[\"candidates\"][0]['place_id']\n",
    "                #print(data_json['candidates'][0][\"geometry\"]['location']['lat'])\n",
    "\n",
    "\n",
    "                #details\n",
    "                details_api ='https://maps.googleapis.com/maps/api/place/details/json?'\n",
    "                url_details = details_api + urllib.parse.urlencode({'place_id':place_id, \n",
    "                                                                     'key': key,'language': language})\n",
    "\n",
    "                time.sleep(2)\n",
    "                details_json = requests.get(url_details).json()\n",
    "                #print('Details:', url_details)\n",
    "                #df.loc[df.index[i], 'Lat'] = data_json['candidates'][0][\"geometry\"]['location']['lat']\n",
    "                #df.loc[df.index[i], 'Lng'] = data_json['candidates'][0][\"geometry\"]['location']['lng']\n",
    "\n",
    "                df.loc[df.index[i], 'Opening_hours'] = abs(int(details_json['result'][\"opening_hours\"][\"periods\"][1]['close']['time'])/100 -\n",
    "                                     int(details_json['result'][\"opening_hours\"][\"periods\"][1]['open']['time'])/100)\n",
    "\n",
    "\n",
    "\n",
    "                #rating, price level\n",
    "                df.loc[df.index[i], 'Rating'] =  data_json['candidates'][0][\"rating\"]\n",
    "                df.loc[df.index[i], 'User_ratings_total'] = data_json['candidates'][0][\"user_ratings_total\"]\n",
    "                df.loc[df.index[i], 'Price_level'] =  data_json['candidates'][0][\"price_level\"]\n",
    "\n",
    "\n",
    "            except KeyError:\n",
    "                continue\n",
    "            except IndexError:\n",
    "                continue\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "#check if there is a road next to store\n",
    "\n",
    "def add_roads(df):\n",
    "    main_api = 'https://roads.googleapis.com/v1/nearestRoads?'\n",
    "\n",
    "    key ='...'\n",
    "   \n",
    "    road_points=[]\n",
    "\n",
    "    df['Долгота'] = df['Долгота'].astype('float')\n",
    "    df['Широта']= df['Широта'].astype('float')\n",
    "    df['Coordinates'] =  list(zip(df['Широта'], df['Долгота']))\n",
    "\n",
    "    for i in  range(0, df.shape[0]):\n",
    "        #print('Query: ',i)\n",
    "        coords = df['Coordinates'].iloc[i]\n",
    "        url = main_api + urllib.parse.urlencode({'points':re.sub(r'\\s*, \\s*', ',', str(coords)[1:-1]),\n",
    "                                                 'key': key})\n",
    "\n",
    "        time.sleep(2)\n",
    "        data_json = requests.get(url).json()\n",
    "        if data_json:\n",
    "            try:\n",
    "                for j in data_json[\"snappedPoints\"]:\n",
    "                    road_points.append((j['location']['latitude'], j['location']['longitude']))\n",
    "\n",
    "                min_dist =  geopy.distance.geodesic(road_points[0], coords).meters\n",
    "                for k in road_points:\n",
    "                    if min_dist> geopy.distance.geodesic(k, coords).meters:\n",
    "                        min_dist = geopy.distance.geodesic(k, coords).meters\n",
    "                df.loc[df.index[i], 'Distance to the nearest road'] = min_dist\n",
    "            except KeyError:\n",
    "                continue\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def main(input_path, input_folder):\n",
    "    \n",
    "    #read data\n",
    "    \n",
    "    poi_data  = pd.read_csv(os.path.join(input_path, input_folder, 'poi.csv'),\n",
    "                                sep =',')\n",
    "    for i in poi_data.columns:\n",
    "        if re.match(r'^Unnamed', i) or re.match(r'^Column', i):\n",
    "            poi_data.drop(i, axis=1, inplace =True)\n",
    "    run.log('initial poi shape:', poi_data.shape)\n",
    "\n",
    "   \n",
    "    store_data = pd.read_csv(os.path.join(input_path, input_folder,'store_char.csv'),  \n",
    "                                sep ='|',\n",
    "                                usecols=[\"Store ID\",\"Active (Y/N)\",\"StoreName\",\"City\",\"Region\",\n",
    "                                'FiasAddress','SourceAddress', 'FiasAccuracy', 'Macroregion',\n",
    "                                'Channel', 'Format',\"Lat/Long\",\"# of Check-outs\",\"Coverage (Y/N)\", \n",
    "                                \"Brick/Sales Territory\",\"# Check-outs for SWB\",\"Chain Name\",\n",
    "                                \"Parent Chain Name\",\"Degree of Freedom\"])\n",
    "    \n",
    "    #functions\n",
    "    prep_data = data_prep(store_data, poi_data)\n",
    "    final_df = prep_data.pipe(add_poi).pipe(add_features).pipe(add_roads)\n",
    "\n",
    "    if 'Opening_hours' in final_df.columns:\n",
    "        final_df['24h'] = [1 if i == 24 else 0 for i in final_df['Opening_hours']]\n",
    "        \n",
    "    if 'Distance to the nearest road' in final_df.columns:\n",
    "        final_df['Nearest_road'] = [1 if i <= 30 else 0 for i in final_df['Distance to the nearest road']]\n",
    "    \n",
    "    final_df.fillna(final_df[[i for i in final_df.columns if  i.islower()]].fillna(0), inplace=True) #fillna cols with google types\n",
    "\n",
    "    if not final_df.empty:\n",
    "        poi_data=pd.concat([poi_data,final_df],ignore_index=True)\n",
    "        poi_data.fillna(poi_data[[i for i in poi_data.columns if  i.islower()] + ['Rating', 'Nearest_road']].fillna(0), inplace=True) #fillna cols with google types\n",
    "\n",
    "    else:\n",
    "        pass\n",
    "    '''\n",
    "    final_df['Точность (ФИАС)'].fillna(store_data['FiasAccuracy'], inplace = True)\n",
    "    final_df['Тип сети'].fillna(store_data['Channel'], inplace = True)\n",
    "    \n",
    "    final_df['Тип сети'] = final_df['Тип сети'].map({'Локальная сеть': 'LKA',  'Национальная сеть': 'NKA'})\n",
    "    final_df.rename(columns ={'Тип сети':'Channel', 'Точность (ФИАС)': 'FiasAccuracy'}, inplace =True)\n",
    "    final_df.drop(['Channel', 'FiasAccuracy', 'Distance to the nearest road'], inplace =True, axis=1)\n",
    "    '''\n",
    "    run.log('final_df shape:', poi_data.shape)\n",
    "\n",
    "    \n",
    "    #save output\n",
    "    poi_data.to_csv(os.path.join(args.input_path, args.input_folder,'poi.csv'),\n",
    "                    encoding='utf-8-sig')\n",
    "   \n",
    "    #----------------------------------------------------------------------------------\n",
    "    \n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    parser = argparse.ArgumentParser('poi step')\n",
    "    parser.add_argument(\"--input_path\", type=str, help=\"input_path\") #data_exp/\n",
    "    parser.add_argument(\"--input_folder\", type=str, help=\"input_folder\") #raw_data\n",
    "    #parser.add_argument(\"--master_data_folder\", type=str, help=\"master_data_folder\") #master_data\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    run.log('Output directory:', os.path.join(args.input_path, args.input_folder))\n",
    "    \n",
    "    \n",
    "    main(args.input_path, args.input_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing mass_explode/photos.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile $script_folder/photos.py\n",
    "\n",
    "#grab photos of store by its address using Places API, save photos to folder in Data Lake\n",
    "\n",
    "from azureml.core import Run\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os, argparse, re, time\n",
    "import urllib\n",
    "import requests, pathlib\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib as jlb # parallelizing\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser('photo')\n",
    "parser.add_argument(\"--input_photo\", type=str, dest=\"input_photo\")\n",
    "parser.add_argument(\"--ncpus\", type=int, default=5, help=\"number of cpus to use\")\n",
    "\n",
    "args = parser.parse_args()\n",
    "\n",
    "path = pathlib.Path(args.input_photo)\n",
    "ds_path = os.path.join(*path.parts[:-3]) \n",
    "\n",
    "run = Run.get_context()\n",
    "\n",
    "df =  pd.read_csv(os.path.join(args.input_photo, 'preped_data.csv'))\n",
    "\n",
    "def add_photo(index, folder):\n",
    "    main_api = 'https://maps.googleapis.com/maps/api/place/findplacefromtext/json?'\n",
    "    key ='...'\n",
    "    language ='ru'\n",
    "    \n",
    "    input = df['Address'].iloc[index]\n",
    "    url = main_api + urllib.parse.urlencode({'input':input, 'inputtype': 'textquery',\n",
    "                                             'key': key,'language': language,\n",
    "                                             'fields': 'place_id,photos,geometry'})\n",
    "    time.sleep(2)\n",
    "    data_json = requests.get(url).json()\n",
    "    if data_json['status'] =='OK':\n",
    "\n",
    "        try:\n",
    "            place_id = data_json[\"candidates\"][0]['place_id']\n",
    "\n",
    "            photo_id = data_json['candidates'][0][\"photos\"][0]['photo_reference']\n",
    "            url_photo = 'https://maps.googleapis.com/maps/api/place/photo?' + urllib.parse.urlencode({'key': key, \n",
    "                                                                                                      'photoreference': photo_id,\n",
    "                                                                                                      'maxwidth':400})\n",
    "\n",
    "            time.sleep(2)\n",
    "            print(url_photo)\n",
    "            img = Image.open(requests.get(url_photo, stream=True).raw) \n",
    "\n",
    "            imgResult = img.resize((256, 256), resample = Image.BILINEAR).convert('RGB')\n",
    "            imgResult.save(folder+'/'+ 'file_' + str(index)+ '.jpg')\n",
    "\n",
    "\n",
    "        except KeyError:\n",
    "            pass\n",
    "\n",
    "\n",
    "jlb.Parallel(n_jobs=args.ncpus)(jlb.delayed(add_photo)(i, \n",
    "                                                       folder= ds_path+ '/Output/MASS/photo') \n",
    "                                for i in  range(0, df.shape[0]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code for a single pipeline run:\n",
    "\n",
    "blob_datastore = Datastore.get(ws, 'rbru') #not ws.datastores['rbru']!\n",
    "\n",
    "data_path = DataPath(datastore= blob_datastore, path_on_datastore='data_exp/')\n",
    "datapath_input = (PipelineParameter(name=\"input_datapath\", default_value=data_path),\n",
    "                  DataPathComputeBinding(mode='mount'))\n",
    "\n",
    "                  #mode='download', path_on_compute=script_folder, overwrite=False)\n",
    "    \n",
    "param_1 = PipelineParameter(name=\"input_folder_param\", default_value='raw_data/') \n",
    "\n",
    "poi_step = PythonScriptStep(\n",
    "    name='poi_center',\n",
    "    source_directory= script_folder, \n",
    "    script_name=\"poi_center.py\",\n",
    "    compute_target=pipeline_run_config.target, \n",
    "    inputs=[datapath_input],\n",
    "    arguments=[\"--input_path\", datapath_input, \n",
    "              \"--input_folder\", param_1],    \n",
    "    runconfig = pipeline_run_config)\n",
    "\n",
    "print(\"poi_step created\")\n",
    "\n",
    "pipeline = Pipeline(workspace=ws, steps=[poi_step])\n",
    "\n",
    "pipeline.validate()\n",
    "\n",
    "# Create an experiment and run the pipeline\n",
    "pipeline_run = Experiment(workspace = ws, \n",
    "                          name = 'poi_center').submit(pipeline, regenerate_outputs=True)\n",
    "\n",
    "#RunDetails(pipeline_run).show()\n",
    "pipeline_run.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "poi: {'Output directory:': '/mnt/batch/tasks/shared/LS_root/jobs/eu-ru-ds-ml-001/azureml/a09a0166-e98e-4a3d-b73e-2cae264b50ea/mounts/rbru/data_exp/raw_data/', 'initial poi shape:': '(14285, 128)', 'final_df shape:': '(14485, 128)'}\n"
     ]
    }
   ],
   "source": [
    "for step_run in pipeline_run.get_children():\n",
    "    print(\"{}: {}\".format(step_run.name, step_run.get_metrics())) #14285;  1139 left for moscow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _schedule pipeline_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "poi_step created\n",
      "Created step poi_volga [5bb93a9c][ddda619c-0f63-494a-94e0-fbd04667c4b2], (This step will run and generate new outputs)\n",
      "Created data reference rbru_5441d7a5 for StepId [0e8f7995][cfd1f3bc-f130-4641-9a76-1dcdf8da730b], (Consumers of this data will generate new runs.)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table style=\"width:100%\"><tr><th>Name</th><th>Id</th><th>Status</th><th>Endpoint</th></tr><tr><td>POI_Pipeline_volga</td><td><a href=\"https://ml.azure.com/pipelines/d39421dd-e759-4690-bb2b-a824d0e731d3?wsid=/subscriptions/6c7359c2-0525-47e1-87c1-f6cb18b47f4f/resourcegroups/RB-EU-RG/workspaces/EU-RU-DS-ML-001\" target=\"_blank\" rel=\"noopener\">d39421dd-e759-4690-bb2b-a824d0e731d3</a></td><td>Active</td><td><a href=\"https://westeurope.api.azureml.ms/pipelines/v1.0/subscriptions/6c7359c2-0525-47e1-87c1-f6cb18b47f4f/resourceGroups/RB-EU-RG/providers/Microsoft.MachineLearningServices/workspaces/EU-RU-DS-ML-001/PipelineRuns/PipelineSubmit/d39421dd-e759-4690-bb2b-a824d0e731d3\" target=\"_blank\" rel=\"noopener\">REST Endpoint</a></td></tr></table>"
      ],
      "text/plain": [
       "Pipeline(Name: POI_Pipeline_volga,\n",
       "Id: d39421dd-e759-4690-bb2b-a824d0e731d3,\n",
       "Status: Active,\n",
       "Endpoint: https://westeurope.api.azureml.ms/pipelines/v1.0/subscriptions/6c7359c2-0525-47e1-87c1-f6cb18b47f4f/resourceGroups/RB-EU-RG/providers/Microsoft.MachineLearningServices/workspaces/EU-RU-DS-ML-001/PipelineRuns/PipelineSubmit/d39421dd-e759-4690-bb2b-a824d0e731d3)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blob_datastore = Datastore.get(ws, 'rbru') \n",
    "\n",
    "data_path = DataPath(datastore= blob_datastore, path_on_datastore='data_exp/')\n",
    "datapath_input = (PipelineParameter(name=\"input_datapath\", default_value=data_path),\n",
    "                  DataPathComputeBinding(mode='mount'))\n",
    "\n",
    "    \n",
    "param_1 = PipelineParameter(name=\"input_folder_param\", default_value='raw_data/') \n",
    "\n",
    "poi_step = PythonScriptStep(\n",
    "    name='poi_volga',\n",
    "    source_directory= script_folder, \n",
    "    script_name=\"poi_volga.py\",\n",
    "    compute_target=pipeline_run_config.target, \n",
    "    inputs=[datapath_input],\n",
    "    arguments=[\"--input_path\", datapath_input, \n",
    "              \"--input_folder\", param_1],    \n",
    "    runconfig = pipeline_run_config)\n",
    "\n",
    "print(\"poi_step created\")\n",
    "pipeline = Pipeline(workspace=ws, steps=[poi_step])\n",
    "\n",
    "published_pipeline = pipeline.publish(name=\"POI_Pipeline_volga\", version=\"1.0\",\n",
    "                                      description=\"adding 200 points every 25 minutes, region: Volga\")\n",
    "\n",
    "published_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.pipeline.core import Schedule, ScheduleRecurrence\n",
    "\n",
    "recurrence = ScheduleRecurrence(frequency=\"Minute\", interval=25)\n",
    "schedule = Schedule.create(ws, name=\"POI_schedule_volga\", pipeline_id=published_pipeline.id,\n",
    "                          experiment_name=\"POI_schedule_volga\", recurrence=recurrence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Run(Experiment: POI_schedule_volga,\n",
      "Id: 06c5886c-c1fc-49bb-82d0-d68f52c5cf1d,\n",
      "Type: azureml.PipelineRun,\n",
      "Status: Running)]\n",
      "2021-03-23T11:56:51.444037Z\n"
     ]
    }
   ],
   "source": [
    "pipelineruns = PipelineRun.get_pipeline_runs(ws, published_pipeline.id)\n",
    "print(pipelineruns)\n",
    "for t in pipelineruns:\n",
    "    details = t.get_details()\n",
    "    print(details['startTimeUtc'])\n",
    "    #print(details['endTimeUtc'])\n",
    "   # print(details['properties']['azureml.parameters'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7da3b420-6899-4583-a0d6-2d8b0b5dc264: poi_volga: {'Output directory:': '/mnt/batch/tasks/shared/LS_root/jobs/eu-ru-ds-ml-001/azureml/0930cb33-fea7-49a8-82ba-30f14f656a0a/mounts/rbru/data_exp/raw_data/', 'initial poi shape:': '(62220, 130)'}\n",
      "61ffffe9-2da8-458c-805d-38b52798d0b5: poi_volga: {'Output directory:': '/mnt/batch/tasks/shared/LS_root/jobs/eu-ru-ds-ml-001/azureml/a09287a7-5a18-4ced-ae16-1e35700d54ba/mounts/rbru/data_exp/raw_data/', 'initial poi shape:': '(62020, 130)', 'final_df shape:': '(62220, 130)'}\n",
      "326bc24f-8319-4536-b677-8d57b8196b6d: poi_volga: {'Output directory:': '/mnt/batch/tasks/shared/LS_root/jobs/eu-ru-ds-ml-001/azureml/22bbde6a-8ae4-4ce4-a98d-5dfed3eabab0/mounts/rbru/data_exp/raw_data/', 'initial poi shape:': '(61820, 130)', 'final_df shape:': '(62020, 130)'}\n",
      "e14e04bb-8d23-4310-bc53-06b3c84fd752: poi_volga: {'Output directory:': '/mnt/batch/tasks/shared/LS_root/jobs/eu-ru-ds-ml-001/azureml/51e544e7-045d-4334-975d-26523b375b71/mounts/rbru/data_exp/raw_data/', 'initial poi shape:': '(61620, 130)', 'final_df shape:': '(61820, 130)'}\n",
      "eeb56902-2066-42df-9974-4a7172367434: poi_volga: {'Output directory:': '/mnt/batch/tasks/shared/LS_root/jobs/eu-ru-ds-ml-001/azureml/fc554db3-7c46-4d75-a0da-d9e667285a5e/mounts/rbru/data_exp/raw_data/', 'initial poi shape:': '(61420, 130)', 'final_df shape:': '(61620, 130)'}\n",
      "b6f69b12-03c6-4745-8760-55bc982840e7: poi_volga: {'Output directory:': '/mnt/batch/tasks/shared/LS_root/jobs/eu-ru-ds-ml-001/azureml/8dc760a0-637c-4442-8bf4-14e8a03639d3/mounts/rbru/data_exp/raw_data/', 'initial poi shape:': '(61220, 130)', 'final_df shape:': '(61420, 130)'}\n",
      "d077881f-e382-4d42-b939-43776370dc3f: poi_volga: {'Output directory:': '/mnt/batch/tasks/shared/LS_root/jobs/eu-ru-ds-ml-001/azureml/a1ba0b0b-f4f5-4c03-90a3-acfe094794d5/mounts/rbru/data_exp/raw_data/', 'initial poi shape:': '(61020, 130)', 'final_df shape:': '(61220, 130)'}\n",
      "c6d855c7-ebf7-4e09-ba90-97246ea56f55: poi_volga: {'Output directory:': '/mnt/batch/tasks/shared/LS_root/jobs/eu-ru-ds-ml-001/azureml/18109962-e301-416f-9dac-0b61ddaa3eb5/mounts/rbru/data_exp/raw_data/', 'initial poi shape:': '(60820, 130)', 'final_df shape:': '(61020, 130)'}\n",
      "223b75bc-7272-4c34-a365-8b0ff9f35680: poi_volga: {}\n",
      "7b839874-3cc5-4967-b6ce-1925d83664c6: poi_volga: {'Output directory:': '/mnt/batch/tasks/shared/LS_root/jobs/eu-ru-ds-ml-001/azureml/34e2cd01-1819-46f7-b07c-9ed56ab7a6e0/mounts/rbru/data_exp/raw_data/', 'initial poi shape:': '(60420, 130)', 'final_df shape:': '(60620, 130)'}\n",
      "cae6e0c5-74c2-470c-b2de-e32c42a42c8a: poi_volga: {'Output directory:': '/mnt/batch/tasks/shared/LS_root/jobs/eu-ru-ds-ml-001/azureml/87cb9efa-8fe9-4382-b87b-733ea02e2ea8/mounts/rbru/data_exp/raw_data/', 'initial poi shape:': '(60220, 130)', 'final_df shape:': '(60420, 130)'}\n",
      "7ddc6930-6bfa-4846-8005-80f83490d155: poi_volga: {'Output directory:': '/mnt/batch/tasks/shared/LS_root/jobs/eu-ru-ds-ml-001/azureml/b34d4cff-1405-4738-adc5-fd439aa2d940/mounts/rbru/data_exp/raw_data/', 'initial poi shape:': '(60020, 130)', 'final_df shape:': '(60220, 130)'}\n",
      "7f71be92-ad5e-4401-83a3-a31fba8a7dfb: poi_volga: {'Output directory:': '/mnt/batch/tasks/shared/LS_root/jobs/eu-ru-ds-ml-001/azureml/014476a5-e421-4ea9-b673-2bf96557efe2/mounts/rbru/data_exp/raw_data/', 'initial poi shape:': '(59820, 130)', 'final_df shape:': '(60020, 130)'}\n",
      "0bb87fc0-5e26-4689-a701-b1bc48b91b3c: poi_volga: {'Output directory:': '/mnt/batch/tasks/shared/LS_root/jobs/eu-ru-ds-ml-001/azureml/c8e37c8a-1486-4719-bbdd-8fb6832a3b81/mounts/rbru/data_exp/raw_data/', 'initial poi shape:': '(59620, 130)', 'final_df shape:': '(59820, 130)'}\n",
      "7beb95fe-7c7b-4a96-89bb-b064b2557855: poi_volga: {'Output directory:': '/mnt/batch/tasks/shared/LS_root/jobs/eu-ru-ds-ml-001/azureml/58b3c29b-77a0-44fa-99b9-16ceb6636ee7/mounts/rbru/data_exp/raw_data/', 'initial poi shape:': '(59420, 130)', 'final_df shape:': '(59620, 130)'}\n",
      "762e33d1-031e-465d-8359-6d40e59dc5ee: poi_volga: {'Output directory:': '/mnt/batch/tasks/shared/LS_root/jobs/eu-ru-ds-ml-001/azureml/ab0fa21c-06df-471b-a085-a6148d3806c8/mounts/rbru/data_exp/raw_data/', 'initial poi shape:': '(59220, 130)', 'final_df shape:': '(59420, 130)'}\n",
      "660e196b-05bc-47e5-b4ce-525324da0da9: poi_volga: {'Output directory:': '/mnt/batch/tasks/shared/LS_root/jobs/eu-ru-ds-ml-001/azureml/8fffb6a7-6177-4fd2-aae2-406ade40bf59/mounts/rbru/data_exp/raw_data/', 'initial poi shape:': '(59020, 130)'}\n",
      "c3b1c038-5f01-4731-b4f4-62d2120296a8: poi_volga: {'Output directory:': '/mnt/batch/tasks/shared/LS_root/jobs/eu-ru-ds-ml-001/azureml/017f0c60-5b2a-46eb-836c-6ac21324d887/mounts/rbru/data_exp/raw_data/', 'initial poi shape:': '(58820, 130)', 'final_df shape:': '(59020, 130)'}\n",
      "7b761629-b2de-479c-a602-b9ba37cdce41: poi_volga: {'Output directory:': '/mnt/batch/tasks/shared/LS_root/jobs/eu-ru-ds-ml-001/azureml/c5996810-a310-4e3a-bf4d-2f7ff734c21c/mounts/rbru/data_exp/raw_data/', 'initial poi shape:': '(58620, 130)', 'final_df shape:': '(58820, 130)'}\n",
      "984f96c6-5cfa-43d6-898f-61595f8065ac: poi_volga: {'Output directory:': '/mnt/batch/tasks/shared/LS_root/jobs/eu-ru-ds-ml-001/azureml/8bcc2013-6af8-4e40-b6fe-c31a4c34d330/mounts/rbru/data_exp/raw_data/', 'initial poi shape:': '(58420, 130)', 'final_df shape:': '(58620, 130)'}\n",
      "0c74685d-a9b2-48bb-96ee-3f8c0b0d8b00: poi_volga: {'Output directory:': '/mnt/batch/tasks/shared/LS_root/jobs/eu-ru-ds-ml-001/azureml/fe53d2e9-83c4-4e77-98d4-c8a8ca4de0dc/mounts/rbru/data_exp/raw_data/', 'initial poi shape:': '(58220, 130)', 'final_df shape:': '(58420, 130)'}\n",
      "3bd1fbf8-6357-4792-990a-7026c04f6d9b: poi_volga: {'Output directory:': '/mnt/batch/tasks/shared/LS_root/jobs/eu-ru-ds-ml-001/azureml/31cf99f7-1205-4281-a3f0-b020808eb0bf/mounts/rbru/data_exp/raw_data/', 'initial poi shape:': '(58020, 130)', 'final_df shape:': '(58220, 130)'}\n",
      "d79ee91f-455a-47ae-a56a-0df709ebaf86: poi_volga: {'Output directory:': '/mnt/batch/tasks/shared/LS_root/jobs/eu-ru-ds-ml-001/azureml/fd3e9d5b-5639-4790-9d99-70d70540ece5/mounts/rbru/data_exp/raw_data/', 'initial poi shape:': '(57820, 130)', 'final_df shape:': '(58020, 130)'}\n",
      "9e47678f-14bc-439d-b6fc-8106221ab87d: poi_volga: {'Output directory:': '/mnt/batch/tasks/shared/LS_root/jobs/eu-ru-ds-ml-001/azureml/7937cdcb-03eb-48df-985c-01aa4e4917b5/mounts/rbru/data_exp/raw_data/'}\n",
      "334c2a57-1b78-4879-b339-c2e9570bee52: poi_volga: {'Output directory:': '/mnt/batch/tasks/shared/LS_root/jobs/eu-ru-ds-ml-001/azureml/fb5d56ac-bd11-44cb-ab77-a878a46a19d5/mounts/rbru/data_exp/raw_data/', 'initial poi shape:': '(57420, 130)', 'final_df shape:': '(57620, 130)'}\n",
      "9dc344e2-4857-459f-ab32-9bcb56914d29: poi_volga: {'Output directory:': '/mnt/batch/tasks/shared/LS_root/jobs/eu-ru-ds-ml-001/azureml/dcfd64d7-20e9-4d4e-8d52-2d12e6ef48d6/mounts/rbru/data_exp/raw_data/', 'initial poi shape:': '(57220, 130)', 'final_df shape:': '(57420, 130)'}\n",
      "261afa4d-56d2-44e9-b20e-b1e72cb6d791: poi_volga: {'Output directory:': '/mnt/batch/tasks/shared/LS_root/jobs/eu-ru-ds-ml-001/azureml/dcceed25-16b4-49a8-aca0-695e28554374/mounts/rbru/data_exp/raw_data/', 'initial poi shape:': '(57020, 130)', 'final_df shape:': '(57220, 130)'}\n",
      "6f1dda9f-5599-408a-8a85-87e311bd9edc: poi_volga: {'Output directory:': '/mnt/batch/tasks/shared/LS_root/jobs/eu-ru-ds-ml-001/azureml/5e47787d-44e8-4780-b4df-84790cb87eac/mounts/rbru/data_exp/raw_data/', 'initial poi shape:': '(56820, 130)', 'final_df shape:': '(57020, 130)'}\n",
      "29eda448-b6ae-4301-b4c8-c6b36a3b81fd: poi_volga: {'Output directory:': '/mnt/batch/tasks/shared/LS_root/jobs/eu-ru-ds-ml-001/azureml/4452cd9e-438a-4a30-b714-e06d685c898a/mounts/rbru/data_exp/raw_data/', 'initial poi shape:': '(56620, 130)', 'final_df shape:': '(56820, 130)'}\n",
      "bda404c5-6327-450c-abef-1b092a98de09: poi_volga: {'Output directory:': '/mnt/batch/tasks/shared/LS_root/jobs/eu-ru-ds-ml-001/azureml/f506367c-afd3-488c-b8c7-ec1cf59bdb53/mounts/rbru/data_exp/raw_data/', 'initial poi shape:': '(56420, 130)', 'final_df shape:': '(56620, 130)'}\n",
      "0a76a719-124e-4e17-8612-5fd2d2a2de18: poi_volga: {'Output directory:': '/mnt/batch/tasks/shared/LS_root/jobs/eu-ru-ds-ml-001/azureml/b0354dff-128d-46dd-b78c-927c16e0512c/mounts/rbru/data_exp/raw_data/', 'initial poi shape:': '(56220, 130)', 'final_df shape:': '(56420, 130)'}\n",
      "8f4fe393-decc-4031-81f9-2fcbe14fe2b2: poi_volga: {'Output directory:': '/mnt/batch/tasks/shared/LS_root/jobs/eu-ru-ds-ml-001/azureml/a9354dc4-420b-49d5-8c87-abfda371b587/mounts/rbru/data_exp/raw_data/', 'initial poi shape:': '(56020, 130)', 'final_df shape:': '(56220, 130)'}\n",
      "58e9105b-3e31-444b-a16d-901b2abcc88c: poi_volga: {'Output directory:': '/mnt/batch/tasks/shared/LS_root/jobs/eu-ru-ds-ml-001/azureml/cb2fb8a0-1d0c-4d63-9606-47a97d26b69d/mounts/rbru/data_exp/raw_data/'}\n",
      "f4a0238f-ad14-4e52-82c2-811bf246e2dd: poi_volga: {'Output directory:': '/mnt/batch/tasks/shared/LS_root/jobs/eu-ru-ds-ml-001/azureml/3f2d2f03-fb2a-4bcd-a44b-d8a5e1232faf/mounts/rbru/data_exp/raw_data/', 'initial poi shape:': '(55820, 130)', 'final_df shape:': '(56020, 130)'}\n",
      "a870e072-cf05-42e7-a346-5e7d628b10cb: poi_volga: {'Output directory:': '/mnt/batch/tasks/shared/LS_root/jobs/eu-ru-ds-ml-001/azureml/d293fac2-3969-428d-9414-77871c5c7d3b/mounts/rbru/data_exp/raw_data/', 'initial poi shape:': '(55620, 130)', 'final_df shape:': '(55820, 130)'}\n",
      "88ef0965-7ade-47b8-ad72-0915aaa3ba13: poi_volga: {'Output directory:': '/mnt/batch/tasks/shared/LS_root/jobs/eu-ru-ds-ml-001/azureml/857453ca-a358-453c-b339-8bd694f600e3/mounts/rbru/data_exp/raw_data/', 'initial poi shape:': '(55420, 130)', 'final_df shape:': '(55620, 130)'}\n",
      "ba72aa09-ad15-4b5d-a713-3c6ecaf3470a: poi_volga: {'Output directory:': '/mnt/batch/tasks/shared/LS_root/jobs/eu-ru-ds-ml-001/azureml/b08c3d99-3dd7-4980-b0c2-90f9ba726b84/mounts/rbru/data_exp/raw_data/', 'initial poi shape:': '(55220, 130)', 'final_df shape:': '(55420, 130)'}\n",
      "3c907d24-50d9-40d2-8f62-0075071ee4af: poi_volga: {'Output directory:': '/mnt/batch/tasks/shared/LS_root/jobs/eu-ru-ds-ml-001/azureml/2efceb0d-84d3-4df2-bb32-c8fd5f25c9bf/mounts/rbru/data_exp/raw_data/', 'initial poi shape:': '(55020, 130)', 'final_df shape:': '(55220, 130)'}\n",
      "aeba968c-6998-4444-a7cf-b65ea55a0e64: poi_volga: {'Output directory:': '/mnt/batch/tasks/shared/LS_root/jobs/eu-ru-ds-ml-001/azureml/e13be430-ff9c-451e-9332-73ba1843d4c5/mounts/rbru/data_exp/raw_data/', 'initial poi shape:': '(54820, 130)', 'final_df shape:': '(55020, 130)'}\n",
      "685c812f-597e-46aa-8de3-2c26c15b5e38: poi_volga: {'Output directory:': '/mnt/batch/tasks/shared/LS_root/jobs/eu-ru-ds-ml-001/azureml/343a2ebb-5edd-4bdd-aac2-43e654b4d3bb/mounts/rbru/data_exp/raw_data/', 'initial poi shape:': '(54620, 130)', 'final_df shape:': '(54820, 130)'}\n",
      "6568e553-70ff-48d2-8600-e7cb6b373c2f: poi_volga: {'Output directory:': '/mnt/batch/tasks/shared/LS_root/jobs/eu-ru-ds-ml-001/azureml/5fba1318-6480-4eb2-bf6f-8025a8662f2e/mounts/rbru/data_exp/raw_data/', 'initial poi shape:': '(54420, 130)', 'final_df shape:': '(54620, 130)'}\n",
      "b5975e6c-a962-4409-ac7a-0d97f6e8de07: poi_volga: {'Output directory:': '/mnt/batch/tasks/shared/LS_root/jobs/eu-ru-ds-ml-001/azureml/b05290a1-8b70-4ed9-84cb-610b73ee4660/mounts/rbru/data_exp/raw_data/', 'initial poi shape:': '(54220, 130)', 'final_df shape:': '(54420, 130)'}\n",
      "55478c64-3c77-4f38-b19f-6d90ee917844: poi_volga: {'Output directory:': '/mnt/batch/tasks/shared/LS_root/jobs/eu-ru-ds-ml-001/azureml/959a2ee4-9230-4867-b75f-b1d44251a1a5/mounts/rbru/data_exp/raw_data/', 'initial poi shape:': '(54020, 130)', 'final_df shape:': '(54220, 130)'}\n",
      "5425d981-ca90-46b3-9557-240e3b04e100: poi_volga: {'Output directory:': '/mnt/batch/tasks/shared/LS_root/jobs/eu-ru-ds-ml-001/azureml/3d735588-4d40-4f16-b24d-7a8938e97a95/mounts/rbru/data_exp/raw_data/', 'initial poi shape:': '(53820, 130)', 'final_df shape:': '(54020, 130)'}\n",
      "68774026-3bed-458c-96d1-f143434cf52f: poi_volga: {'Output directory:': '/mnt/batch/tasks/shared/LS_root/jobs/eu-ru-ds-ml-001/azureml/f5ac87a8-bdb7-4748-a38e-5d6eb0c946dd/mounts/rbru/data_exp/raw_data/', 'initial poi shape:': '(53620, 130)', 'final_df shape:': '(53820, 130)'}\n",
      "a670d0ad-f14c-4cfb-9d3f-4b6a4033aee0: poi_volga: {'Output directory:': '/mnt/batch/tasks/shared/LS_root/jobs/eu-ru-ds-ml-001/azureml/c6952221-c6a5-4062-b95a-1db1ba0ba6fc/mounts/rbru/data_exp/raw_data/', 'initial poi shape:': '(53420, 130)', 'final_df shape:': '(53620, 130)'}\n",
      "bbdae4f0-9e9a-4f70-a5b3-f571250ef4ad: poi_volga: {'Output directory:': '/mnt/batch/tasks/shared/LS_root/jobs/eu-ru-ds-ml-001/azureml/be51f865-c7ba-4add-afb7-da0c29b4aef7/mounts/rbru/data_exp/raw_data/', 'initial poi shape:': '(53220, 130)', 'final_df shape:': '(53420, 130)'}\n",
      "9f263075-2145-4c35-ba5a-7e09461b30a1: poi_volga: {'Output directory:': '/mnt/batch/tasks/shared/LS_root/jobs/eu-ru-ds-ml-001/azureml/9ef35814-5eb2-4773-9699-81e3199ba064/mounts/rbru/data_exp/raw_data/', 'initial poi shape:': '(53020, 130)', 'final_df shape:': '(53220, 130)'}\n",
      "ddd6dfbb-3dcf-4a65-96a5-4658604f1c81: poi_volga: {'Output directory:': '/mnt/batch/tasks/shared/LS_root/jobs/eu-ru-ds-ml-001/azureml/6966a538-bc6d-472f-9ee1-de2e8678175e/mounts/rbru/data_exp/raw_data/', 'initial poi shape:': '(52820, 130)', 'final_df shape:': '(53020, 130)'}\n",
      "286ca739-afaa-47f3-9f4e-a6739dd8d93b: poi_volga: {'Output directory:': '/mnt/batch/tasks/shared/LS_root/jobs/eu-ru-ds-ml-001/azureml/bf46cf8c-508a-41f6-86c3-a500da4a829e/mounts/rbru/data_exp/raw_data/', 'initial poi shape:': '(52620, 130)', 'final_df shape:': '(52820, 130)'}\n",
      "05eb8ed0-47d7-4d1c-b2f8-8115c1c87ff4: poi_volga: {'Output directory:': '/mnt/batch/tasks/shared/LS_root/jobs/eu-ru-ds-ml-001/azureml/6d2679c6-a295-4063-9723-3ae1a55b5e89/mounts/rbru/data_exp/raw_data/', 'initial poi shape:': '(52420, 130)', 'final_df shape:': '(52620, 130)'}\n",
      "51e58e70-3077-43ae-9702-30d62878e665: poi_volga: {'Output directory:': '/mnt/batch/tasks/shared/LS_root/jobs/eu-ru-ds-ml-001/azureml/dc2e6192-9962-4f1f-ba9c-5f662be79cca/mounts/rbru/data_exp/raw_data/', 'initial poi shape:': '(52220, 130)', 'final_df shape:': '(52420, 130)'}\n",
      "18d12260-5602-4701-b9f3-9993be462d8f: poi_volga: {'Output directory:': '/mnt/batch/tasks/shared/LS_root/jobs/eu-ru-ds-ml-001/azureml/8aacfc5a-4611-469d-a4b6-6ab24569ca13/mounts/rbru/data_exp/raw_data/', 'initial poi shape:': '(52020, 130)', 'final_df shape:': '(52220, 130)'}\n",
      "51d8188f-fce2-4c10-89fa-f069944874c4: poi_volga: {'Output directory:': '/mnt/batch/tasks/shared/LS_root/jobs/eu-ru-ds-ml-001/azureml/0dbfa5b4-88de-4795-86e2-c96711293f36/mounts/rbru/data_exp/raw_data/', 'initial poi shape:': '(51820, 130)', 'final_df shape:': '(52020, 130)'}\n",
      "6fb8ddcc-732e-4238-8f02-3cf5c71073a3: poi_volga: {'Output directory:': '/mnt/batch/tasks/shared/LS_root/jobs/eu-ru-ds-ml-001/azureml/9ab95e4f-5411-4bd2-974d-93d5d0395ae4/mounts/rbru/data_exp/raw_data/', 'initial poi shape:': '(51620, 130)', 'final_df shape:': '(51820, 130)'}\n",
      "59360193-aeac-4b6d-b018-329c0bc31582: poi_volga: {'Output directory:': '/mnt/batch/tasks/shared/LS_root/jobs/eu-ru-ds-ml-001/azureml/7e2f65d4-5fd4-4922-8dd0-42b6ef24579c/mounts/rbru/data_exp/raw_data/', 'initial poi shape:': '(51420, 130)', 'final_df shape:': '(51620, 130)'}\n",
      "81ec48b5-d719-4704-8632-5fe3d1704d0d: poi_volga: {'Output directory:': '/mnt/batch/tasks/shared/LS_root/jobs/eu-ru-ds-ml-001/azureml/f6aee64b-b8b8-4a2c-82fa-1d59f17284aa/mounts/rbru/data_exp/raw_data/', 'initial poi shape:': '(51220, 130)', 'final_df shape:': '(51420, 130)'}\n",
      "0b7a1b2e-8877-493c-8331-fa10232fdb17: poi_volga: {'Output directory:': '/mnt/batch/tasks/shared/LS_root/jobs/eu-ru-ds-ml-001/azureml/94f302a3-534a-42c0-8e41-9fcb034bc7ad/mounts/rbru/data_exp/raw_data/', 'initial poi shape:': '(51020, 130)', 'final_df shape:': '(51220, 130)'}\n",
      "6b16a592-330b-4e9d-80d7-b9dbcd6872ff: poi_volga: {'Output directory:': '/mnt/batch/tasks/shared/LS_root/jobs/eu-ru-ds-ml-001/azureml/a4983c2e-e026-431d-ab79-243f4c9a7688/mounts/rbru/data_exp/raw_data/', 'initial poi shape:': '(50820, 130)', 'final_df shape:': '(51020, 130)'}\n",
      "fdfc21c8-a077-4649-b93d-5b8f78a7d04a: poi_volga: {'Output directory:': '/mnt/batch/tasks/shared/LS_root/jobs/eu-ru-ds-ml-001/azureml/80f7d0be-6cc9-444e-b2ed-b0d11655b790/mounts/rbru/data_exp/raw_data/', 'initial poi shape:': '(50620, 130)', 'final_df shape:': '(50820, 130)'}\n",
      "98eb8a13-6e1a-43ec-957a-63d1602c31c6: poi_volga: {'Output directory:': '/mnt/batch/tasks/shared/LS_root/jobs/eu-ru-ds-ml-001/azureml/d0a82caf-f488-4c15-8ed1-49bfed866334/mounts/rbru/data_exp/raw_data/', 'initial poi shape:': '(50420, 130)', 'final_df shape:': '(50620, 130)'}\n",
      "12e7a1fc-4e9b-4193-a346-0523abfc57ff: poi_volga: {'Output directory:': '/mnt/batch/tasks/shared/LS_root/jobs/eu-ru-ds-ml-001/azureml/aaed5aff-4157-42e9-bca6-95ef93d6071b/mounts/rbru/data_exp/raw_data/', 'initial poi shape:': '(50220, 130)', 'final_df shape:': '(50420, 130)'}\n",
      "2430d6f9-b5e2-4f7e-b577-30e7ab760a6e: poi_volga: {'Output directory:': '/mnt/batch/tasks/shared/LS_root/jobs/eu-ru-ds-ml-001/azureml/d62becf0-69f1-4969-9dc3-cd3ca10840e1/mounts/rbru/data_exp/raw_data/', 'initial poi shape:': '(50020, 130)', 'final_df shape:': '(50220, 130)'}\n",
      "1aa7a649-abc7-4d20-84e8-67c55db532a2: poi_volga: {'Output directory:': '/mnt/batch/tasks/shared/LS_root/jobs/eu-ru-ds-ml-001/azureml/daf9d5b1-1bb4-4e13-829b-e044b71a2174/mounts/rbru/data_exp/raw_data/', 'initial poi shape:': '(49820, 130)', 'final_df shape:': '(50020, 130)'}\n",
      "85c2193e-8763-4644-981d-a9f40d95df37: poi_volga: {'Output directory:': '/mnt/batch/tasks/shared/LS_root/jobs/eu-ru-ds-ml-001/azureml/edd94d0c-3dc2-4b0b-b2d2-18073b3447f9/mounts/rbru/data_exp/raw_data/', 'initial poi shape:': '(49620, 130)', 'final_df shape:': '(49820, 130)'}\n",
      "85e76f2d-4d48-4c76-a726-21bbabebfc82: poi_volga: {'Output directory:': '/mnt/batch/tasks/shared/LS_root/jobs/eu-ru-ds-ml-001/azureml/6c750f3b-cccf-4d97-9908-a57c7e80218f/mounts/rbru/data_exp/raw_data/', 'initial poi shape:': '(49420, 130)', 'final_df shape:': '(49620, 130)'}\n",
      "dfb29c33-611a-4b08-8ad4-0c493851ba5c: poi_volga: {'Output directory:': '/mnt/batch/tasks/shared/LS_root/jobs/eu-ru-ds-ml-001/azureml/04265f61-9534-4893-b238-56b9043d2520/mounts/rbru/data_exp/raw_data/', 'initial poi shape:': '(49220, 130)', 'final_df shape:': '(49420, 130)'}\n",
      "06c5886c-c1fc-49bb-82d0-d68f52c5cf1d: poi_volga: {'Output directory:': '/mnt/batch/tasks/shared/LS_root/jobs/eu-ru-ds-ml-001/azureml/c340720d-ee81-4e70-a42d-c6608be87240/mounts/rbru/data_exp/raw_data/', 'initial poi shape:': '(49020, 130)', 'final_df shape:': '(49220, 130)'}\n"
     ]
    }
   ],
   "source": [
    "for i in schedule.get_pipeline_runs(): \n",
    "    for step_run in i.get_children():\n",
    "        print(\"{}: {}: {}\".format(i.id, step_run.name, step_run.get_metrics()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Pipeline(Name: POI_schedule_south,\n",
       " Id: 94ac63e3-d9f7-48af-a266-a5394fa95a32,\n",
       " Status: Active,\n",
       " Pipeline Id: 5427d66b-7557-4c66-a886-0445d301cf04,\n",
       " Recurrence Details: Runs every 25 Minutes)]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ss = Schedule.list(ws)\n",
    "ss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _enable/ disable schedule_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found these schedules for the pipeline id d39421dd-e759-4690-bb2b-a824d0e731d3:\n",
      "1419cda1-8535-44f1-98ed-e5495510af43\n",
      "Schedule id: 1419cda1-8535-44f1-98ed-e5495510af43\n",
      "Using schedule with id: 1419cda1-8535-44f1-98ed-e5495510af43\n",
      "Provisioning status: Completed\n",
      "Disabled schedule 1419cda1-8535-44f1-98ed-e5495510af43. New status is: Disabled\n"
     ]
    }
   ],
   "source": [
    "#all schedules for this pipeline\n",
    "schedules = Schedule.list(ws, pipeline_id= published_pipeline.id)\n",
    "print(\"Found these schedules for the pipeline id {}:\".format(published_pipeline.id))\n",
    "\n",
    "\n",
    "for schedule in schedules: \n",
    "    print(schedule.id)\n",
    "    if schedule.recurrence is not None:\n",
    "        schedule_id = schedule.id\n",
    "print(\"Schedule id: {}\".format(schedule_id))\n",
    " \n",
    "  #get schedule by id\n",
    "fetched_schedule = Schedule.get(ws, schedule_id)\n",
    "print(\"Using schedule with id: {}\".format(fetched_schedule.id))\n",
    "\n",
    "\n",
    "#disable this schedule (by id)\n",
    "\n",
    "fetched_schedule.disable(wait_for_provisioning=True)\n",
    "fetched_schedule = Schedule.get(ws, schedule_id)\n",
    "print(\"Disabled schedule {}. New status is: {}\".format(fetched_schedule.id, fetched_schedule.status))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 560,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Provisioning status: Completed\n",
      "Updated schedule: 76466852-b1bf-4668-bd5e-e522ed03c437 \n",
      "New name: POI_schedule_north_west \n",
      "New frequency: Minute \n",
      "New status: Active\n"
     ]
    }
   ],
   "source": [
    "#change reccurence\n",
    "\n",
    "recurrence = ScheduleRecurrence(frequency=\"Minute\", interval=45) # Runs every 45 minutes\n",
    "\n",
    "fetched_schedule = Schedule.get(ws, schedule_id)\n",
    "\n",
    "fetched_schedule.update(name=\"POI_schedule_north_west\", \n",
    "                        description=\"Updated_POI_schedule_north_west\", \n",
    "                        status='Active', \n",
    "                        wait_for_provisioning=True,\n",
    "                        recurrence=recurrence)\n",
    "\n",
    "fetched_schedule = Schedule.get(ws, fetched_schedule.id)\n",
    "\n",
    "print(\"Updated schedule:\", fetched_schedule.id, \n",
    "      \"\\nNew name:\", fetched_schedule.name,\n",
    "      \"\\nNew frequency:\", fetched_schedule.recurrence.frequency,\n",
    "      \"\\nNew status:\", fetched_schedule.status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline(Name: POI_schedule,\n",
      "Id: f0b7f2ac-18cc-43e2-881f-a2fe7c4b6b06,\n",
      "Status: Disabled,\n",
      "Pipeline Id: 286065c9-2a98-4ae5-8756-0118ee3c0d79,\n",
      "Recurrence Details: Runs every 45 Minutes)]\n"
     ]
    }
   ],
   "source": [
    "#disable all\n",
    "ss = Schedule.list(ws)\n",
    "for s in ss:\n",
    "    s.disable()\n",
    "print(ss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
